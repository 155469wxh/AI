# 分类与回归算法-机器学习领域的hellow world

## K-means.K最邻近算法
### 步骤
1.确定K值(多少个样本簇中心)
K过小导致过拟合
K过大导致欠拟合和过于泛化
2.计算距离(通常使用欧氏距离，可以有多种计算方式)
3.找到最近的K个点
4.投票
查看K个邻居点的标签，将新数据点分配给K个邻居中最常见的类别
5.结果，得到了新数据点的预测类型
### 优缺点
1.优点
>算法简单直观
无需建模
无需训练

2.缺点
>计算量大
维数灾难
对于样本不平衡问题的敏感性

## 回归算法
### 简介
检查两个或两个以上变量之间的关系
1.线性回归
2.岭回归
线性回归的正则化版本，可以避免过拟合
3.LASSO回归
4.逻辑回归
5.多项式回归，可以拟合非线性回归
### 算法解读
1.基本原理$$Y=kX+b$$ 
2.寻找最佳拟合直线
3.最小化误差
调整拟合直线参数
4.多元线性回归
### 注意问题
1.验证线性关系的假设
2.考虑异常值的影响
3.注意多重共线性问题
### 线性回归的衍生算法

#### LASSO回归(引入正则项)
##### 理解
通过向线性回归中引入L1正则化项实现特征选择
##### 公式
$$J(\theta)=MSE+\lambda\sum_{i=1}^{p}\left|\theta_{i}\right|$$
##### 参数理解
$\lambda$是正则化强度的超参数
$\theta$是模型参数
p是特征的数量
MSE是模型在训练数据上的均方误差
##### 原理解释
L1正则化是对参数进行约束的方式
L1正则化的工作原理是实现稀疏性
L1正则化倾向于忽略模型中不重要的参数，有助于模型忽略不重要的特征
#### 岭回归
##### 公式
$$J(\theta)=MSE+\lambda\sum_{i=1}^{p}\theta_{i}^{2}$$
##### 原理解释
L2正则化的工作原理是对权重参数施加惩罚
组织模型过于依赖训练数据中的某一个特征，***增强模型的泛化能力***

#### LASSO回归于岭回归的异同
##### 相同点
1.防止过拟合
2.设置惩罚参数
3.注重模型泛化能力
##### 不同点
1.对权重的处理方式
>LASSO能对很多权重直接归零
岭回归中权重不为零，但是接近于零

2.参数控制方式
>LASSO通过绝对值来控制，得到精确为零的权重
岭回归用平方值来实现，倾向于得到接近于零的权重

3.对异常值的敏感性
>LASSO对异常值更加鲁棒(更不敏感)
岭回归对异常值更加敏感

4.应用场景
>LASSO对于无序的稀疏数据更加有效
岭回归对数据之间关联性较强的数据集更有效
#### 逻辑回归
##### 简单解释
将线性回归的输出结果映射到(0,1)之间，表示为概率
将回归任务变成分类任务(最适合二分类模型)
约等于sigmoid函数(逻辑函数)+线性回归
这是一种有监督算法
##### 优点
>1.输出概率评分，对于许多应用场景很有价值
2.简单且易于理解
3.计算效率高
##### 缺点
>1.表达能力有限
2.特征依赖严重。假设的线性关系可能并不合理，甚至根本不成立
3.易受到异常值的影响
#### 多项式回归
##### 公式
$$y=\beta_{0}+\beta_{1}x+\beta_{2}x^{2}+\beta_{3}x^{3}+\beta_{4}x^{4}+\boldsymbol{b}$$
##### 算法优势
1.非线性关系的表达
2.模型的拟合能力
3.参数的数量和复杂度



